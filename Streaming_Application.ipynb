{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1a43cd5f270e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Run stream for 15 minutes just in case no detection of producer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;31m# ssc.awaitTermination()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopSparkContext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstopGraceFully\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.0 pyspark-shell'\n",
    "\n",
    "# Importing required libraries\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import ast\n",
    "import pyspark.sql as ps\n",
    "\n",
    "# function to send recieved messages from producer to db\n",
    "def sendDataToDB(iter):\n",
    "    client = MongoClient() #initialising client\n",
    "    db = client.fit5148_db # creating Db\n",
    "    climate = db.climate # Creating collection\n",
    "    i = 0\n",
    "    hotspot_list =[]\n",
    "    climate_list = []\n",
    "    for record in iter:\n",
    "        data = ast.literal_eval(record[1]) # Converts the input message to dictionary\n",
    "        if data[\"id\"]=='sender_1':\n",
    "            climate_list.append(data)    # creating two lists for climate and hotspot records        \n",
    "        else:\n",
    "            hotspot_list.append(data)\n",
    "    # Algorithm :  We will check if the stream has more than one hotspot record, if yes we will check if the records \n",
    "    # have same geohash, if yes we will average surface temperature and confidence and append only one record to db. If no, we \n",
    "    # will discard the hotspot record.But we will update the climate record.\n",
    "    if(len(hotspot_list)>1): \n",
    "        if(hotspot_list[0]['geohash']!=hotspot_list[1]['geohash']):\n",
    "         \n",
    "            for i in range(len(hotspot_list)):\n",
    "                for j in range(len(climate_list)):\n",
    "                    k = hotspot_list[i]['geohash']\n",
    "                    l = climate_list[j]['geohash']\n",
    "                    if(k == l):\n",
    "                        climate_list[j]['fire_record'] = hotspot_list[i]\n",
    "                        #hotspot.insert_one(hotspot_list[i])\n",
    "                        break\n",
    "        else:\n",
    "            x= {}\n",
    "            y = 0\n",
    "            k = 0\n",
    "            #if equal geohash,averaging confidence & surface_temperature\n",
    "            for i in range(len(hotspot_list)):\n",
    "                y = y +int(hotspot_list[i]['confidence'])\n",
    "                k = k + int(hotspot_list[i]['surface_temperature_celcius'])\n",
    "\n",
    "            x['confidence']=y/2\n",
    "            x['surface_temperature_celcius']=k/2\n",
    "            x['latitude'] =hotspot_list[0]['latitude']\n",
    "            x['longitude'] = hotspot_list[0]['longitude']\n",
    "            for j in range(len(climate_list)):\n",
    "                    k = hotspot_list[0]['geohash']\n",
    "                    l = climate_list[j]['geohash']\n",
    "                    if(k == l):\n",
    "                        climate_list[j]['fire_record'] = x\n",
    "                        break\n",
    "    #Checking of it has only one record and matching geo hash key generated                  \n",
    "    elif(len(hotspot_list) == 1):\n",
    "        for j in range(len(climate)):\n",
    "            k = hotspot_list[0]['geohash']\n",
    "            l = climate_list[j]['geohash']\n",
    "            if(k == l):\n",
    "                climate_list[j]['fire_record'] = hotspot_list[0]\n",
    "                break\n",
    "    #finally inserting the records into Db            \n",
    "    try:\n",
    "        climate.insert_many(climate_list)\n",
    "        hotspot_list = []\n",
    "        climate_list = []\n",
    "    except Exception as ex:\n",
    "        print(\"Exception Occured. Message: {0}\".format(str(ex)))\n",
    "                    \n",
    "    client.close()\n",
    "\n",
    "    \n",
    "\n",
    "n_secs = 10 # proving the batch size as 10\n",
    "topic = \"Scenario01\" # topicname same as in producers\n",
    "\n",
    "conf = SparkConf().setAppName(\"KafkaStreamProcessor\").setMaster(\"local[2]\")  # setting two local threads\n",
    "sc = SparkContext.getOrCreate() # creating spark context\n",
    "if sc is None:\n",
    "    sc = SparkContext(conf=conf)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "ssc = StreamingContext(sc, n_secs) #initialising the streaming context\n",
    "    \n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {\n",
    "                        'bootstrap.servers':'127.0.0.1:9092', \n",
    "                        'group.id':'week11-group', \n",
    "                        'fetch.message.max.bytes':'15728640',\n",
    "                        'auto.offset.reset':'largest'})\n",
    "                        # Group ID is completely arbitrary\n",
    "\n",
    "lines = kafkaStream.foreachRDD(lambda rdd: rdd.foreachPartition(sendDataToDB))\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(900) # Run stream for 15 minutes just in case no detection of producer\n",
    "# ssc.awaitTermination()\n",
    "ssc.stop(stopSparkContext=True,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
